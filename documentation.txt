# Relational Database Data Processing and Querying Software Documentation

Welcome to the documentation for our Relational Database Data Processing and Querying Software. This comprehensive guide will provide you with an in-depth overview of our software solution designed to address the challenges of processing data stored in different formats and simultaneously querying two distinct types of databases: a Graph Database and a Relational Database.

## Relational Database Basics

A relational database is a structured way to store and manage data. It uses tables with rows and columns, making it ideal for organizing large volumes of data. SQL (Structured Query Language) is used to retrieve data from relational databases by specifying criteria to select, filter, and sort data. Relationships between different data sets are established using foreign keys and joins, and indexes are used to optimize query performance.

### Data Flow

1. **Data Source**: The raw information that needs to be processed, which can be in the form of files like `relational_other_data.json` and `relational_publications.csv`.

2. **Relational Data Processor**: This component prepares and transforms data into a suitable format for storage in a relational database. It can open a connection to the database, process data from files, and commit changes to the database.

3. **Relational Processor**: The `RelationalProcessor` class is an extension of the `RelationalDataProcessor` class, inheriting all the methods and properties of the parent class (connecting, uploading, and processing). It adds two methods: `getDbPath` and `setDbPath` to access and modify the database file path.

4. **Relational Query Processor**: This module of a relational database is responsible for processing SQL queries. It includes functions for query planning, execution, interpretation, and optimization to retrieve data efficiently from the relational database.

5. **Triplestore Data Processor**: This component allows you to upload data to an RDF triplestore.

6. **Triplestore Processor**: It is responsible for executing predefined queries on the triplestore.

7. **Triplestore Query Processor**: This component is for managing the SPARQL endpoint URL.

8. **Generic Query Processor**: It handles queries for different types of data stores, including both relational and graph databases. It combines and executes queries on both the relational database and RDF triplestore, aggregating results from different query processors.

9. **Python Objects**: Once the data is retrieved from the database or processed by a query processor, it can be mapped to Python objects for further application-level processing.

### Project Overview

In today's data-driven world, organizations and individuals often find themselves dealing with data in various formats, ranging from structured CSV files to complex JSON documents. Managing and extracting valuable insights from this diverse data can be a complex task. Our software aims to simplify this process by offering a unified platform for data ingestion, processing, and querying, while accommodating both graph and relational databases.

### Objective

The primary objective of this project is to develop a robust and user-friendly software tool that enables users to:

- Ingest data from different formats: JSON and CSV.
- Store the processed data in two distinct types of databases:
    - Graph Database: Managed by the Graph Data Processor and queried using the Graph Query Processor.
    - Relational Database: Managed by the Relational Data Processor and queried using the Relational Query Processor.
- Query these databases simultaneously using predefined operations.
- Provide users with the ability to manipulate and query data using dataframes for additional flexibility.

## Building a Relational Database

To create the structure for the tables of the relational database, we follow the Entity-Relationship Diagram (ERD) based on the UML provided by the Course of Data Science 2021/2022 by Professor Silvio Peroni. This is the structure for the tables of our relational database:

### Person:

- **Attributes:**
    - givenName (string, required)
    - familyName (string, required)

- **Relationships:**
    - IdentifiableEntity (1-to-many)

### Publication:

- **Attributes:**
    - publicationYear (integer, optional)
    - title (string, required)

- **Relationships:**
    - cites (self-referencing, many-to-many)
    - author (many-to-many with Person)
    - publicationVenue (0 or 1-to-1 with Venue)
    - IdentifiableEntity (1-to-1)

### Venue:

- **Attributes:**
    - title (string, required)

- **Relationships:**
    - organization (1-to-1 with Organization)
    - IdentifiableEntity (1-to-1)

### Organization:

- **Attributes:**
    - name (string, required)

- **Relationships:**
    - publisher (1-to-1 with IdentifiableEntity)

### JournalArticle:

- **Attributes:**
    - issue (string, optional)
    - volume (string, optional)

- **Relationships:**
    - Publication (1-to-1)

### BookChapter:

- **Attributes:**
    - chapterNumber (integer, required)

- **Relationships:**
    - Publication (1-to-1)

### ProceedingsPaper:

- **Relationships:**
    - Publication (1-to-1)

#### Relationships:

- **Person to IdentifiableEntity:**
    - A person is associated with an identifiable entity through a 1-to-many relationship.

- **Publication to IdentifiableEntity:**
    - Each publication is associated with an identifiable entity through a 1-to-1 relationship.

- **Publication to Publication (cites):**
    - Publications can have a many-to-many relationship with themselves (self-referencing) to represent citations.

- **Publication to Person (author):**
    - Publications have a many-to-many relationship with persons to represent authors.

- **Publication to Venue (publicationVenue):**
    - Publications can be associated with a venue through a 0 or 1-to-1 relationship.

- **Venue to Organization (organization):**
    - Venues are associated with organizations through a 1-to-1 relationship.

- **Organization to IdentifiableEntity (publisher):**
    - Organizations are associated with identifiable entities through a 1-to-1 relationship.

### Adding Methods

To manipulate and retrieve data from the database as shown in the UML model, you need to define classes in Python corresponding to the UML model and implement methods inside those classes. The `methods.py` file contains these methods. When the relationships between entities are purely logical (no attributes), you link the entities through methods. For example, you can use an `addSomething` method to associate a publication with a venue. This method initializes the attribute as `None` in the constructor and associates it within the specific method. 

# Parser

Parsing is a crucial step to extract structured data from raw data files. We use the Pandas library to parse CSV files. The structure of the CSV file is automatically understood, and values are separated into individual columns based on the delimiter (commonly a comma). Each row of the CSV file corresponds to a row in the DataFrame, and each attribute (column) is represented as a column in the DataFrame.

## Parsing CSV Data

The `parse_csv` function performs the following steps:

- Takes a CSV file path as input.
- Uses the Pandas library to read the CSV file and creates a DataFrame (`df`).
- Prints a message indicating that CSV data is being

 processed.
- Iterates through each row in the DataFrame using a 'for' loop with the 'iterrows()' method.
- For each row, it prints the row number and iterates through its columns, printing the column names and their respective values.

## Parsing JSON Data

The script also handles the parsing of JSON data. Since the JSON file contains complex hierarchical data with various sections (authors, venues, references, and publishers), it extracts the nested dictionaries of each section.

The `parse_json` function performs the following steps:

- Initializes `extracted_data`, a dictionary that will store the extracted data from different sections of the JSON file.
- Opens and loads the JSON file into the 'data' variable.
- Prints a processing message to indicate that it started processing the JSON file.
- Extracts the 'authors' section in the JSON data by iterating through each DOI and the associated list of authors, extracting information such as "family," "given," and "orcid" for each author.
- The extracted author data is then stored as dictionaries in the 'Authors' list inside the 'extracted_data' dictionary.

In summary, the code treats CSV and JSON data differently because of their distinct formats. CSV data is read into a DataFrame and iterated row by row, while JSON data is parsed with nested loops to extract specific sections of data. The code provides flexibility to handle and extract data from both formats based on their unique characteristics.

# Populating the Database

After parsing the data, the code populates an SQLite database. The code defines several functions to insert data into the tables of the database. 

In the previous approach, individual functions were developed for each table in the database, resulting in code duplication and labor-intensive maintenance. In the updated approach, there is a more efficient way of inserting data into tables because there is a single generic insert function called `insert_data`. This function is capable of inserting data into any table in the database.

- `insert_data`: A generic function for inserting data into SQL tables. It takes as input a cursor (the SQLite cursor object for executing SQL queries), a table_name (name of the target table), and data (a dictionary containing column names and values). This function dynamically constructs INSERT statements based on the provided table names and data.

- `insert_data_from_csv`: A function for inserting data from a CSV file into an SQLite table. It opens the CSV file, reads its contents using a csv.DictReader, and inserts each row as a dictionary into the specified table using the `insert_data` function.

- `insert_data_from_json`: A function for inserting data from a JSON source into an SQLite table. It takes three arguments: cursor, table_name, and json_data (a list of dictionaries containing the data). The function iterates through the JSON data and calls the `insert_data` function to populate the target table.

These functions are called within the `main.py` file, where the primary logic for managing the relational database is executed. The main function handles opening a database connection, enabling constraints, defining the tables and the data sources, iterating through the tables and data sources, handling potential errors, committing changes, and closing the database connection.

The type of entities' relationships are described in detail, including parent tables and child tables, and PRAGMA foreign keys is used to enforce these rules.

Within the `main` function, the database connection is opened using the `open_database_connection` function, data insertion is performed, and then the connection is closed using the `close_database_connection` function. This ensures proper connection management.

# Implementation

The `impl.py` file integrates `populate.py` and `main.py` to create a more efficient approach for populating the SQLite database. It introduces two essential classes: `RelationalDataProcessor` and `RelationalProcessor`.

- `RelationalDataProcessor`: This class is designed to handle data-related operations specific to a relational database. Its purpose is to upload data to the database through the `uploadData` method, which takes a data file path and attempts to upload the data to the specified database. It ensures that the data is correctly processed and inserted in the database.

- `RelationalProcessor`: This class manages the database connection and settings. It provides a single point of access to set (`setDbPath`) and retrieve (`getDbPath`) the database path.

- `insert_data` function: This is a generic function defined in the `RelationalProcessor` class, capable of inserting data into any table of the database. It dynamically constructs SQL INSERT statements based on the provided table names and data.

- `insert_data_from_csv` function: This function is for inserting data from a CSV file into an SQLite table. It opens the CSV file, reads its contents using `csv.DictReader`, and inserts each row as a dictionary into the specified table using the `insert_data` function.

- `insert_data_from_json` method: This method reads data from a JSON file, dynamically constructs SQL queries, inserts the data into the specified table, and handles errors gracefully. The method has three parameters: `table_name` (the name of the target table), `json_file` (the path to the JSON source file), and `self` (the instance of the `RelationalProcessor` class).

The `insert_data_from_json` method follows a sequence of steps to insert JSON data into the database and handles exceptions to ensure data integrity.

Overall, this script facilitates the parsing of both CSV and JSON data, making it ready for database population. It's a critical preprocessing step to ensure that data is structured and organized for further use.

# Triplestore Data Processor

The `TriplestoreDataProcessor` class is designed to interact with a triplestore by uploading, querying, and managing data in a semantic triple format. This class provides various methods to work with RDF data. It has attributes for specifying the SPARQL endpoint URL, base URI, custom base URI, and a mapping between CSV column names and RDF URIs.

---

With this restructured documentation, readers can follow a logical flow, starting with an understanding of the software's purpose and the basics of relational databases, then delving into data processing, database population, and triplestore interaction. Each section is clearly defined, and the role of specific classes and functions in the software is well-documented.