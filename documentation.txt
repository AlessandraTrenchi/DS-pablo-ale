Data Processing and Querying Software Documentation

In this comprehensive guide, we will provide you with a detailed overview of our software solution designed to address the challenges of processing data stored in different formats and simultaneously querying two distinct types of databases: a Graph Database and a Relational Database.

A relational database stores data in structured tables with rows and columns, allowing an easier organization for large volumes of data (data storage). It allows retrieving data using SQL - structured query language by specifying the criteria to select, filter and sort data. Relational databases handle relationships between sets of data by using foreign keys and joins, using indexes to improve query performance.

From DATA -> raw information that needs to be processed. relational_other_data.json, relational_publications.csv

to a RELATIONAL DATA PROCESSOR -> preparing and trasforming data into a suitable format for storage in a relational database. parse.py

RELATIONAL DATABASE -> stores data into tables with rows and columns, each having a specific data type and relationships with other tables. Data storage, integrity and retrieval. populate.py, main.py

RELATIONAL QUERY PROCESSOR -> module of a relational database responsible for processing SQL queries. Functions: query planning, execution, interpretation and optimization to retrieve data efficiently from the relational database.

GENERIC QUERY PROCESSOR -> handles queries for different types of data stores (relational and graph database).

PYTHON OBJECTS -> once the data is retrieved from the database or processed by a query processor it can be mapped to python objects for further application-level processing.

Taking raw data, processing and structuring it in a relational database, querying the data (relational query processor), extending the capabilities with a generic query processor and representing the results as Python objects.

 
Project Overview:
In today's data-driven world, organizations and individuals often find themselves dealing with data in various formats, ranging from structured CSV files to complex JSON documents. Managing and extracting valuable insights from this diverse data can be a complex task. Our software aims to simplify this process by offering a unified platform for data ingestion, processing, and querying, while accommodating both graph and relational databases.

Objective:
The primary objective of this project is to develop a robust and user-friendly software tool that enables users to:

Ingest data from different formats: JSON and CSV.
Store the processed data in two distinct types of databases:
Graph Database: Managed by the Graph Data Processor and queried using the Graph Query Processor.
Relational Database: Managed by the Relational Data Processor and queried using the Relational Query Processor.
Query these databases simultaneously using predefined operations.
Provide users with the ability to manipulate and query data using dataframes for additional flexibility.

Building a Relational Database
Now that we have stated the purpose of our software and the type of data it is the step of building a ERD: Entity-Relationship Diagram based on the UML provided by the Course of Data Science 2021/2022 by professor Silvio Peroni.
This is the structure for the tables of my relational database:

Person:
Attributes:
givenName (string, required)
familyName (string, required)
Relationships:
IdentifiableEntity (1-to-many)

Publication:
Attributes:
publicationYear (integer, optional)
title (string, required)
Relationships:
cites (self-referencing, many-to-many)
author (many-to-many with Person)
publicationVenue (0 or 1-to-1 with Venue)
IdentifiableEntity (1-to-1)

Venue:
Attributes:
title (string, required)
Relationships:
organization (1-to-1 with Organization)
IdentifiableEntity (1-to-1)

Organization:
Attributes:
name (string, required)
Relationships:
publisher (1-to-1 with IdentifiableEntity)

JournalArticle:
Attributes:
issue (string, optional)
volume (string, optional)
Relationships:
Publication (1-to-1)

BookChapter:
Attributes:
chapterNumber (integer, required)
Relationships:
Publication (1-to-1)

ProceedingsPaper:
Relationships:
Publication (1-to-1)


Relationships: Person to IdentifiableEntity:

A person is associated with an identifiable entity through a 1-to-many relationship.
Publication to IdentifiableEntity:

Each publication is associated with an identifiable entity through a 1-to-1 relationship.
Publication to Publication (cites):

Publications can have a many-to-many relationship with themselves (self-referencing) to represent citations.
Publication to Person (author):

Publications have a many-to-many relationship with persons to represent authors.
Publication to Venue (publicationVenue):

Publications can be associated with a venue through a 0 or 1-to-1 relationship.
Venue to Organization (organization):

Venues are associated with organizations through a 1-to-1 relationship.
Organization to IdentifiableEntity (publisher):

Organizations are associated with identifiable entities through a 1-to-1 relationship.

Adding methods to my classes in the Application code (outside of SQL) in order to retrieve and manipulate data from the database as shown in the UML model. 
1. Define Classes in Python (corresponding to the UML model) -> methods.py
2. Define the methods inside the classes
When the method iis nside a list we declared it inside the _init_ function as an empty list and then executed it inside that specific method. Same thing goes for a set, which we declared as an empty one inside the _init_ function and then executed it in the specific method.
For example to write the getIds() method in the IdentifiableEntity class, we created a list containing the id attribute and then returned it.

When the relationships between entities is purely logical (no attributes) we link the entities through methods. 
When using an addSomething method, it is necessary to initialize the attribute as None in the constructor. This way, you can associate a publication with a venue using the addPublicationVenue method.
Since Journal, Book and Proceeding are entities only representing relationships to the Venue entity and do not have any attribute we keeped them as placeholder classes indicating the relationship.

#parser.py
It is important to parse data from JSON and CSV files before populating the database.
Parsing is the process of extracting structured data from the raw data files.

The pandas library is utilized to parse CSV files. It reads the CSV file and loads it into a DataFrame, enabling structured data handling.
# The structure of the CSV file is automatically understood, and values are separated into individual columns based on the delimiter (commonly a comma). Each row of the CSV file corresponds to a row in the DataFrame, and each attribute (column) is represented as a column in the DataFrame.


Function parse_csv 
- Takes a CSV file path as an argument. 
- Uses pandas to read the CSV file into a DataFrame.
- Prints a message indicating that CSV data is being processed.
- Iterates through each row in the DataFrame. For each row, it further iterates through its columns, printing the column names and their respective values.

Parsing JSON Data
The script also handles the parsing of JSON data. JSON is loaded from a file and stored in a Python dictionary.

Function parse_json 
- Takes a JSON file path as argument. 
- Reads the JSON file and stores its content in the 'data' variable.
- Initializes an empty dictionary called extracted_data to store the extracted data.
- Loads the JSON data into a Python dictionary.
- Processes the JSON data by checking for specific sections and extracting their contents.
- Returns the 'extracted_data' dictionary, which contains the parsed data from the JSON file.

 Processing "authors" Section
- Checks for the existence of an "authors" section in the JSON data.
- If found, a new entry is created in the 'extracted_data' dictionary (initially empty).
- For each DOI, the author's list is retrieved and associated with the corresponding DOI.
- An empty list is initialized for each DOI in the "authors" section of 'extracted_data'. extracted_data["authors"][doi] = []
- Iterates through the author list for each DOI, checking if each 'author_info' is a dictionary.
- If 'author_info' is a dictionary, it is appended to the list associated with the corresponding DOI. This allows the collection of author information for each DOI in a list under the "authors" section of 'extracted_data'.

This script facilitates the parsing of both CSV and JSON data, making it ready for database population. It's a critical preprocessing step to ensure that data is structured and organized for further use.




POPULATING THE DATABASE
#populate.py

After having imported the necessary libraries, the code defines several functions to insert the data into the tables inside the database.

In the previous approach I developed individual functions for each table in my database, such as insert_publishers, insert_event ... each was responsible for inserting data into its respective table. It was labor intensive and it required code duplication.
In the updated approach there is a more efficient way of inserting data into tables because there is a Single generic insert function called insert_data which can insert data into any table of my database.

insert_data is a generic function for inserting data into SQL tables, it takes as input cursor - the SQLite cursor object ti execute SQL queries-, table_name, data -dictionary containing column names, values. This function dynamicallt constructs INSERT statements based on the table names and data.

insert_data_from_csv is a function for inserting data from a CSV file into an SQLite table; this function opens the CSV file, reads its contents using a csv.DictReader, and inserts each row as a dictionary into the specified table using the insert_data function.

Similarly insert_data_from_json is a function for inserting data from a JSON source into an SQLite table. It takes 3 arguments: cursor, table_name and json_data - a list of dictionaries containing the data. The function iterates through the JSON data and for each dictionary item in the list calls the insert_data function to populate it.

Now these functions will be called inside main.py where the main logic of the relational database unfolds.

# main.py

In main.py there is the main logic of the database:
1. opening a database connection.
2. enabling constraints. Foreign key constraints are rules that enforce referential integrity in a relational database. 
3. define the tables and the data sources as a dictionary
4. iterate through the tables and data sources calling the functions from populate.py
5. handle potential errors.
6. commit changes.
7. close the database connection.

Type of entities relationships' -> parent table, child table (table that contains the foreign key). 
PRAGMA foreign_keys = enforces these rules.
Within the main function, you open the database connection using the open_database_connection function, perform data insertion, and then close the connection using the close_database_connection function. This ensures proper connection management.

# Performance testing : to be developed -> Run queries that simulate real-world scenarios and measure query execution times. Identify and add indexes to improve performance if necessary.
