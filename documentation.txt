Data Processing and Querying Software Documentation

In this comprehensive guide, we will provide you with a detailed overview of our software solution designed to address the challenges of processing data stored in different formats and simultaneously querying two distinct types of databases: a Graph Database and a Relational Database.

A relational database stores data in structured tables with rows and columns, allowing an easier organization for large volumes of data (data storage). It allows retrieving data using SQL - structured query language by specifying the criteria to select, filter and sort data. Relational databases handle relationships between sets of data by using foreign keys and joins, using indexes to improve query performance.

From DATA -> raw information that needs to be processed. relational_other_data.json, relational_publications.csv

to a RELATIONAL DATA PROCESSOR -> preparing and trasforming data into a suitable format for storage in a relational database. parse.py

RELATIONAL DATABASE -> stores data into tables with rows and columns, each having a specific data type and relationships with other tables. Data storage, integrity and retrieval. populate.py, main.py

RELATIONAL QUERY PROCESSOR -> module of a relational database responsible for processing SQL queries. Functions: query planning, execution, interpretation and optimization to retrieve data efficiently from the relational database.

GENERIC QUERY PROCESSOR -> handles queries for different types of data stores (relational and graph database).

PYTHON OBJECTS -> once the data is retrieved from the database or processed by a query processor it can be mapped to python objects for further application-level processing.

Taking raw data, processing and structuring it in a relational database, querying the data (relational query processor), extending the capabilities with a generic query processor and representing the results as Python objects.

 
Project Overview:
In today's data-driven world, organizations and individuals often find themselves dealing with data in various formats, ranging from structured CSV files to complex JSON documents. Managing and extracting valuable insights from this diverse data can be a complex task. Our software aims to simplify this process by offering a unified platform for data ingestion, processing, and querying, while accommodating both graph and relational databases.

Objective:
The primary objective of this project is to develop a robust and user-friendly software tool that enables users to:

Ingest data from different formats: JSON and CSV.
Store the processed data in two distinct types of databases:
Graph Database: Managed by the Graph Data Processor and queried using the Graph Query Processor.
Relational Database: Managed by the Relational Data Processor and queried using the Relational Query Processor.
Query these databases simultaneously using predefined operations.
Provide users with the ability to manipulate and query data using dataframes for additional flexibility.

Building a Relational Database
Now that we have stated the purpose of our software and the type of data it is the step of building a ERD: Entity-Relationship Diagram based on the UML provided by the Course of Data Science 2021/2022 by professor Silvio Peroni.
This is the structure for the tables of my relational database:

Person:
Attributes:
givenName (string, required)
familyName (string, required)
Relationships:
IdentifiableEntity (1-to-many)

Publication:
Attributes:
publicationYear (integer, optional)
title (string, required)
Relationships:
cites (self-referencing, many-to-many)
author (many-to-many with Person)
publicationVenue (0 or 1-to-1 with Venue)
IdentifiableEntity (1-to-1)

Venue:
Attributes:
title (string, required)
Relationships:
organization (1-to-1 with Organization)
IdentifiableEntity (1-to-1)

Organization:
Attributes:
name (string, required)
Relationships:
publisher (1-to-1 with IdentifiableEntity)

JournalArticle:
Attributes:
issue (string, optional)
volume (string, optional)
Relationships:
Publication (1-to-1)

BookChapter:
Attributes:
chapterNumber (integer, required)
Relationships:
Publication (1-to-1)

ProceedingsPaper:
Relationships:
Publication (1-to-1)


Relationships: Person to IdentifiableEntity:

A person is associated with an identifiable entity through a 1-to-many relationship.
Publication to IdentifiableEntity:

Each publication is associated with an identifiable entity through a 1-to-1 relationship.
Publication to Publication (cites):

Publications can have a many-to-many relationship with themselves (self-referencing) to represent citations.
Publication to Person (author):

Publications have a many-to-many relationship with persons to represent authors.
Publication to Venue (publicationVenue):

Publications can be associated with a venue through a 0 or 1-to-1 relationship.
Venue to Organization (organization):

Venues are associated with organizations through a 1-to-1 relationship.
Organization to IdentifiableEntity (publisher):

Organizations are associated with identifiable entities through a 1-to-1 relationship.

Adding methods to my classes in the Application code (outside of SQL) in order to retrieve and manipulate data from the database as shown in the UML model. 
1. Define Classes in Python (corresponding to the UML model) -> methods.py
2. Define the methods inside the classes
When the method iis nside a list we declared it inside the _init_ function as an empty list and then executed it inside that specific method. Same thing goes for a set, which we declared as an empty one inside the _init_ function and then executed it in the specific method.
For example to write the getIds() method in the IdentifiableEntity class, we created a list containing the id attribute and then returned it.

When the relationships between entities is purely logical (no attributes) we link the entities through methods. 
When using an addSomething method, it is necessary to initialize the attribute as None in the constructor. This way, you can associate a publication with a venue using the addPublicationVenue method.
Since Journal, Book and Proceeding are entities only representing relationships to the Venue entity and do not have any attribute we keeped them as placeholder classes indicating the relationship.

#parser.py
It is important to parse data from JSON and CSV files before populating the database.
Parsing is the process of extracting structured data from the raw data files.

The pandas library is utilized to parse CSV files. It reads the CSV file and loads it into a DataFrame, enabling structured data handling.
# The structure of the CSV file is automatically understood, and values are separated into individual columns based on the delimiter (commonly a comma). Each row of the CSV file corresponds to a row in the DataFrame, and each attribute (column) is represented as a column in the DataFrame.


Function parse_csv 
- Takes a CSV file path as input. 
- Uses pandas library to read the CSV file and creates a DataFrame (df).
- Prints a message indicating that CSV data is being processed.
- Iterates through each row in the DataFrame using a 'for' loop with the 'iterrows()' method. 
- For each row, it prints the row number, then it further iterates through its columns, printing the column names and their respective values.

Parsing JSON Data
The script also handles the parsing of JSON data. 
Since the JSON file contains complex hierarchical data with various sections (authors, venues, references and publishers) it extracts the nested dictionaries of each section.

parse_json(json_file)
 - Initialise extracted_data: a dictionary that will store the extracted data from different sections of the JSON file.
 - Open and load the JSON file into the 'data' variable.
 - Print a processing message to indicate that it started processing the JSON file.
 - Extract the 'authors' section in the JSON data, by iterating through each DOI and the associated list of authors, extracting information "family", "given" and "orcid" for each author.
 - The extracted author data is then stored as dictionaries in the Authors list inside the extracted_data dictionary.
 
In summary, the code treats CSV and JSON data differently because of their distinct formats. CSV data is read into a DataFrame and iterated row by row, while JSON data is parsed with nested loops to extract specific sections of data. The code provides flexibility to handle and extract data from both formats based on their unique characteristics.

 Processing "authors" Section
- Checks for the existence of an "authors" section in the JSON data.
- If found, a new entry is created in the 'extracted_data' dictionary (initially empty).
- For each DOI, the author's list is retrieved and associated with the corresponding DOI.
- An empty list is initialized for each DOI in the "authors" section of 'extracted_data'. extracted_data["authors"][doi] = []
- Iterates through the author list for each DOI, checking if each 'author_info' is a dictionary.
- If 'author_info' is a dictionary, it is appended to the list associated with the corresponding DOI. This allows the collection of author information for each DOI in a list under the "authors" section of 'extracted_data'.

This script facilitates the parsing of both CSV and JSON data, making it ready for database population. It's a critical preprocessing step to ensure that data is structured and organized for further use.




POPULATING THE DATABASE
#populate.py

After having imported the necessary libraries, the code defines several functions to insert data into the tables of a SQLite database.

In the previous approach individual functions were developed for each table in the database, such as insert_publishers, insert_event ... each was responsible for inserting data into its respective table resulting in code duplication and labor-intensive maintenance.

In the updated approach there is a more efficient way of inserting data into tables because there is a single generic insert function called insert_data. This function is capable of inserting data into any table in the database.

insert_data is a generic function for inserting data into SQL tables, it takes as input cursor - the SQLite cursor object for executing SQL queries-, table_name (name of the target table), data -dictionary containing column names, values. This function dynamically constructs INSERT statements based on the provided table names and data.

insert_data_from_csv is a function for inserting data from a CSV file into an SQLite table. It opens the CSV file, reads its contents using a csv.DictReader, and inserts each row as a dictionary into the specified table using the insert_data function.

Similarly insert_data_from_json is a function for inserting data from a JSON source into an SQLite table. It takes 3 arguments: cursor, table_name and json_data - a list of dictionaries containing the data. The function iterates through the JSON data and for each dictionary item in the list calls the insert_data function to populate the target table.

These functions are now called within main.py, where the primary logic for managing the relational database is executed.

# main.py

In main.py there is the main logic of the database:
1. opening a database connection.
2. enabling constraints. Foreign key constraints are rules that enforce referential integrity in a relational database. 
3. define the tables and the data sources as a dictionary
4. iterate through the tables and data sources calling the functions from populate.py
5. handle potential errors.
6. commit changes.
7. close the database connection.

Type of entities relationships' -> parent table, child table (table that contains the foreign key). 
PRAGMA foreign_keys = enforces these rules.
Within the main function, you open the database connection using the open_database_connection function, perform data insertion, and then close the connection using the close_database_connection function. This ensures proper connection management.

#methods.py

MAYBE IN METHODS DIVIDE BASED ON THE NAME OF THE PROCESSOR, SO MORE MODULARITY



#impl.py

I have integrated populate.py and main.py into impl.py to create a more efficient approach for populating my SQLite database.

RelationalDataProcessor Class -> designed to handle data-related operations specific to a relational dataabase. It has the purpose of uploading data to the database through the 'uploadData' method which takes a data file path and attempts to upload the data to the specified database. It ensures that the data is correctly processed and inserted in the database.

RelationalProcessor Class -> manages the database connection and settings. It provides a single point of access to set (setDbPath) and retrieve (getDbPath) the database path. 

insert_data Function: generic function defined in RelationalProcessor class, capable of inserting data into any table of the database. It dynamically constructs SQL INSERT statements based on the provided table names and data.

A function named insert_data_from_csv is defined in the RelationalProcessor class. It opens a CSV file, reads its contents using csv.DictReader, and inserts each row as a dictionary into the specified table using the insert_data function.

The insert_from_json method the insert_data_from_json method reads data from a JSON file, dynamically constructs SQL queries, inserts the data into the specified table, and handles errors gracefully. The method has three parameters: table_name (the name of the target table), json_file (the path to the JSON source file), and self (the instance of the RelationalProcessor class).
- It starts by performing some input validation to ensure that the necessary data is provided. If the database path is not set or if the JSON file does not exist, it returns False to indicate that the operation failed.
- It establishes a connection to the SQLite database. This is done using the sqlite3.connect method and creating a cursor for executing SQL queries.
- The method opens the JSON file using with open(json_file, 'r') as json_file and reads the content into a Python data structure using json.load(json_file). This results in a list of dictionaries, where each dictionary represents a row of data to be inserted into the table.
- It then iterates through each dictionary item in the list, which represents a row of data.
- For each item, it constructs an SQL INSERT query dynamically based on the table name and the keys in the dictionary (which correspond to column names).
- It generates placeholders for the values using placeholders = ', '.join(['?'] * len(item)), creating as many placeholders as there are values in the row.
- The query variable is constructed with the INSERT statement to insert the data into the specified table. It includes the column names and placeholders for the values.
- It then executes the query with the data values using cursor.execute(query, list(item.values())). This inserts the data from the dictionary into the table.
- After all data has been inserted, the method calls conn.commit() to commit the changes to the database.
- If an exception occurs during this process, the method catches it, prints an error message, and returns False to indicate that the operation failed. If all goes well, it returns True to indicate that the data insertion was successful.





# Performance testing : to be developed -> Run queries that simulate real-world scenarios and measure query execution times. Identify and add indexes to improve performance if necessary.

# CHECK IF I HAVE TABLES IN MY DATABASE.SQL THAT ARE NOT POPULATED YOU DO NOT HAVE TO INCLUDE THEM IN THE TABLE LIST INSIDE POPULATE.PY

NEED TO ADD A VENUE_TYPE TABLE IN DATABASE.SQL and a way to extract its data, am i doing it already?


# CHECK IF THE TABLES HAVE RELATIONSHIPS CHILD-PARENT -> If you have a table that is part of a bigger table (a child table), and the child table's data is derived from the same data source as the parent table, you don't necessarily need to insert the child table in the table_sources dictionary separately. Instead, you can handle it within the code that populates the parent table (in this case, the "Publication" table) from the data source ("relational_publications.csv").