Data Processing and Querying Software Documentation

In this comprehensive guide, we will provide you with a detailed overview of our software solution designed to address the challenges of processing data stored in different formats and simultaneously querying two distinct types of databases: a Graph Database and a Relational Database.

Project Overview:
In today's data-driven world, organizations and individuals often find themselves dealing with data in various formats, ranging from structured CSV files to complex JSON documents. Managing and extracting valuable insights from this diverse data can be a complex task. Our software aims to simplify this process by offering a unified platform for data ingestion, processing, and querying, while accommodating both graph and relational databases.

Objective:
The primary objective of this project is to develop a robust and user-friendly software tool that enables users to:

Ingest data from different formats: JSON and CSV.
Store the processed data in two distinct types of databases:
Graph Database: Managed by the Graph Data Processor and queried using the Graph Query Processor.
Relational Database: Managed by the Relational Data Processor and queried using the Relational Query Processor.
Query these databases simultaneously using predefined operations.
Provide users with the ability to manipulate and query data using dataframes for additional flexibility.

Building a Relational Database
Now that we have stated the purpose of our software and the type of data it is the step of building a ERD: Entity-Relationship Diagram based on the UML provided by the Course of Data Science 2021/2022 by professor Silvio Peroni.
This is the structure for the tables of my relational database:

Person:
Attributes:
givenName (string, required)
familyName (string, required)
Relationships:
IdentifiableEntity (1-to-many)

Publication:
Attributes:
publicationYear (integer, optional)
title (string, required)
Relationships:
cites (self-referencing, many-to-many)
author (many-to-many with Person)
publicationVenue (0 or 1-to-1 with Venue)
IdentifiableEntity (1-to-1)

Venue:
Attributes:
title (string, required)
Relationships:
organization (1-to-1 with Organization)
IdentifiableEntity (1-to-1)

Organization:
Attributes:
name (string, required)
Relationships:
publisher (1-to-1 with IdentifiableEntity)

JournalArticle:
Attributes:
issue (string, optional)
volume (string, optional)
Relationships:
Publication (1-to-1)

BookChapter:
Attributes:
chapterNumber (integer, required)
Relationships:
Publication (1-to-1)

ProceedingsPaper:
Relationships:
Publication (1-to-1)


Relationships: Person to IdentifiableEntity:

A person is associated with an identifiable entity through a 1-to-many relationship.
Publication to IdentifiableEntity:

Each publication is associated with an identifiable entity through a 1-to-1 relationship.
Publication to Publication (cites):

Publications can have a many-to-many relationship with themselves (self-referencing) to represent citations.
Publication to Person (author):

Publications have a many-to-many relationship with persons to represent authors.
Publication to Venue (publicationVenue):

Publications can be associated with a venue through a 0 or 1-to-1 relationship.
Venue to Organization (organization):

Venues are associated with organizations through a 1-to-1 relationship.
Organization to IdentifiableEntity (publisher):

Organizations are associated with identifiable entities through a 1-to-1 relationship.

Adding methods to my classes in the Application code (outside of SQL) in order to retrieve and manipulate data from the database as shown in the UML model. 
1. Define Classes in Python (corresponding to the UML model) -> methods.py
2. Define the methods inside the classes
When the method iis nside a list we declared it inside the _init_ function as an empty list and then executed it inside that specific method. Same thing goes for a set, which we declared as an empty one inside the _init_ function and then executed it in the specific method.
For example to write the getIds() method in the IdentifiableEntity class, we created a list containing the id attribute and then returned it.

When the relationships between entities is purely logical (no attributes) we link the entities through methods. 
When using an addSomething method, it is necessary to initialize the attribute as None in the constructor. This way, you can associate a publication with a venue using the addPublicationVenue method.
Since Journal, Book and Proceeding are entities only representing relationships to the Venue entity and do not have any attribute we keeped them as placeholder classes indicating the relationship.

#parser.py
It is important to parse data from JSON and CSV files before populating the database.
Parsing is the process of extracting structured data from the raw data files.

The pandas library reads the CSV file and loads it into the DataFrame in order to work with data in a structured way. 
Pandas library understands the structure of the file and separates the values into individual columns based on the delimiter (a comma). Each row of the CSV file becomes a row in the DataFrame, and each attribute (column) is represented as a column in the DataFrame.
parse_csv takes a CSV file path as an argument. Inside the function:
- We use pandas to read the CSV file into a DataFrame.
- We print a message to indicate that we're processing CSV data.
- We iterate through each row in the DataFrame and, for each row, iterate through its columns to print the column names and values.

function parse_json that takes a JSON file path as an argument. Inside the function:

- We open and read the JSON file using the json library.
- We print a message to indicate that we're processing JSON data.
- We check if the JSON data is a list (signifying multiple objects) or a single object:
If it's a list, we iterate through each object and print the key-value pairs.
If it's not a list, we print a message indicating that the JSON data format is not supported.


POPULATING THE DATABASE
#populate.py

After having imported the necessary libraries, the code defines several functions to insert the data into the tables inside the database.

insert_data is a generic function for inserting data into SQL tables, it takes as input cursor - the SQLite cursor object ti execute SQL queries-, table_name, data -dictionary containing column names, values. This function dynamicallt constructs INSERT statements based on the table names and data.

insert_data_from_csv is a function for inserting data from a CSV file into an SQLite table; this function opens the CSV file, reads its contents using a csv.DictReader, and inserts each row as a dictionary into the specified table using the insert_data function.

Similarly insert_data_from_json is a function for inserting data from a JSON source into an SQLite table. It takes 3 arguments: cursor, table_name and json_data - a list of dictionaries containing the data. The function iterates through the JSON data and for each dictionary item in the list calls the insert_data function to populate it.


All functions take a cursor object (used for SQL commands) and the specified data as parameters. They build a SQL INSERT query to insert data into a specific table with the provided values.

The insert_data_from_csv function inserts the data from a CSV file into the database by taking the cursor and CSV path files as arguments. First it opens the CSV file with csv.reader, then skips the header row because it contains the column names. The function iterates through the rows in the CSV file, calls the insert_publisher function to insert data in the appropriate table. If there is an error during insertion it gives an error message. The insert_data_from_json function works similarly.

In the previous approach I developed individual functions for each table in my database, such as insert_publishers, insert_event ... each was responsible for inserting data into its respective table. It was labor intensive and it required code duplication.
In the updated approach there is a more efficient way of inserting data into tables because there is a Single generic insert function called insert_data which can insert data into any table of my database.

Inside the loop use SQL INSERT statements to insert the extracted values into the respective tables.
After inserting the data for each row, commit the changes to the database and close the database connection.



# Performance testing : to be developed -> Run queries that simulate real-world scenarios and measure query execution times. Identify and add indexes to improve performance if necessary.
